---
layout: post
title: Dimensionality Reduction, Kernel PCA and LDA
date: 2022-01-11 08:40:16
description: Kernel explanation and LDA
tags: machinelearning
categories: notes
toc: true
---

## Kernel PCA

PCA is a linear method. That is it can only be applied to datasets which are linearly separable.Kernel PCA uses a kernel function to project dataset into a higher dimensional feature space, where it is linearly separable [source](https://www.geeksforgeeks.org/ml-introduction-to-kernel-pca/)

`Why High Dimension?`: VC (Vapnik-Chervonenkis) theory tells us
that often mappings which take us into a
higher dimensional space than the
dimension of the input space provide us
with greater classification power [source](http://www.cs.haifa.ac.il/~rita/uml_course/lectures/KPCA.pdf)

{% include figure.html path="assets/img/lda5.png" class="img-fluid rounded z-depth-1" %}

But if we simply map the data into higher dimensions, the computation cost would be also high. Why?:

{% include figure.html path="assets/img/lda6.png" class="img-fluid rounded z-depth-1" %}

Here comes the `Kernel Trick`: Get the benefit of high-dimension, but avoid the computation burden brought by high dimensions.

Given any algorithm that can be expressed solely **in terms of dot products**, this trick allows us to construct different nonlinear versions of it. And it will save compuational cost. Why?

{% include figure.html path="assets/img/lda7.png" class="img-fluid rounded z-depth-1" %}

Popular Kernel funnctions:

- Polynomial kernel $$ k(x,y)=(x^Ty+1)^d $$. It looks not only at the given features of input samples to determine their similarity, but also combinations of these. With $$n$$ original features and $$d$$ degrees of polynomial, the polynomial kernel yields $$n^d$$ expanded features.
- RBF kernel(Gaussian kernel) $$ k(x,y)=e^{-\gamma||x-y||^2} $$. There is an infinite number of dimensions in the feature space because it can be expanded by the Taylor Series. The $$\gamma$$ parameter defines how much influence a single training example has. The larger it is, the closer other examples must be to be affected

`Math`: [source](http://www.cs.haifa.ac.il/~rita/uml_course/lectures/KPCA.pdf)

`Limitation`: Overfitting when we mapping features into higher dimensions.

## Linear Discriminant Analysis

`Main Idea`: With tagged data, we can perform supervised dimensionality reduction. The LDA is to project all the data points on to a lower dimension, which seperates the classes as far as possible. [source1](https://medium.com/analytics-vidhya/linear-discriminant-analysis-explained-in-under-4-minutes-e558e962c877), [source2](https://medium.com/@viveksalunkhe80/linear-discriminant-analysis-2b7bfc409f9b)

{% include figure.html path="assets/img/lda1.jpeg" class="img-fluid rounded z-depth-1" %}

We achieve this by maximizing **between-class variance** and minimizing **within-class variance**.

{% include figure.html path="assets/img/lda2.png" class="img-fluid rounded z-depth-1" %}

{% include figure.html path="assets/img/lda3.png" class="img-fluid rounded z-depth-1" %}

Fisher defined the separation between these two distributions to be the ratio of the variance between the classes to the variance within the classes.

`What we can get from LDA?`:

Low-dimensional representations of data points, where each class could be largely seperated.

{% include figure.html path="assets/img/lda3.png" class="img-fluid rounded z-depth-1" %}

`Use Case`: Reduce the number of features to a more manageable number before classification

`Limitation` LDA will fail if discriminatory information is not in the mean but in the variance of the data (That is, the classes share mean value.). We can use **Non-linear Discriminant Analysis**:[source](https://medium.com/@viveksalunkhe80/linear-discriminant-analysis-2b7bfc409f9b)

1. Quadratic Discriminant Analysis (QDA): Each class uses its own estimate of variance (or covariance when there are multiple input variables).
2. Flexible Discriminant Analysis (FDA): Where non-linear combinations of inputs is used such as splines.
3. Regularized Discriminant Analysis (RDA): Introduces regularization into the estimate of the variance (actually covariance), moderating the influence of different variables on LDA.
