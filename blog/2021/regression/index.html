<!DOCTYPE html>
<html lang="en">

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Manqiu  L.


  | Regression Training, Regularization and Odds

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåå</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://manqiul.github.io/blog/2021/regression/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://manqiul.github.io/">
       <span class="font-weight-bold">Manqiu</span>   L.
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/courses/">
                Courses
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/experiences/">
                Experiences
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item dropdown ">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Interests
                
              </a>
              <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
              
              
                <a class="dropdown-item" href="/photography/">Photography</a>
              
              
              
                <div class="dropdown-divider"></div>
              
              
              
                <a class="dropdown-item" href="/reading/">Reading</a>
              
              
              </div>
          </li>
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      





<div class="post">

  <header class="post-header">
    <h2 class="post-title">Regression Training, Regularization and Odds</h2>
    <p class="post-meta">December 25, 2021</p>
    <p class="post-tags">
      <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a>
      
      ¬† ¬∑ ¬†
        
        <a href="/blog/tag/machinelearning">
          <i class="fas fa-hashtag fa-sm"></i> machinelearning</a> ¬†
          
      

      
      ¬† ¬∑ ¬†
        
        <a href="/blog/category/notes">
          <i class="fas fa-tag fa-sm"></i> notes</a> ¬†
          
      

    </p>
  </header>

  <article class="post-content">
    <h3 id="linear-regression">Linear Regression</h3>

<p><strong>1. Closed Form</strong>
<a href="https://brunomaga.github.io/Supervised-Learning" target="_blank" rel="noopener noreferrer">source</a></p>

<p>We‚Äôd like to minimize the loss function, which in linear regression we use MSE:</p>

\[\begin{aligned}
(y‚àíXw)^2&amp;=(y‚àíXw)^T(y‚àíXw)\\
&amp;=y^Ty‚àíy^TXw-(Xw)^Ty+(Xw)^TXw\\
&amp;=y^Ty - w^TX^Ty-w^TX^Ty+w^TX^TXw\\
&amp;=y^Ty-2w^TX^Ty+w^TX^TXw;
\end{aligned}\]

<p>Then we take the derivative, and get</p>

\[0-2X^Ty+2X^TXw=0\\\]

<p>which gives us:</p>

\[w=(X^TX)^{-1}X^Ty\\\]

<p>As it takes a lot to calculate the inverse of $X^TX$, we can use pseudo inverse and matrix factorization instead.</p>

<p><strong>2. Pseudo Inverse</strong>
<a href="https://en.wikipedia.org/wiki/Proofs_involving_the_Moore%E2%80%93Penrose_inverse#A+_=_(A*_A)+A*" target="_blank" rel="noopener noreferrer">source1</a>
<a href="https://spartanideas.msu.edu/2015/10/21/regression-via-pseudoinverse/" target="_blank" rel="noopener noreferrer">source2</a>
<a href="https://mathformachines.com/posts/least-squares-with-the-mp-inverse/" target="_blank" rel="noopener noreferrer">source3</a></p>

<p>By definition, pseudoinverse of matrix \(X^+=(X^TX)^{-1}X^T\). So if the $X$ is invertible, using pseudoinverse \(w=X^+y\) to calculate $w$ would be the same as the closed form.</p>

<ul>
  <li>Note: in this case, to reduce the calculation cost, we can use singular vector decomposition \(X = UŒ£V^T\) to calculate \(X^+\), which is \(X^+ = ùëâŒ£^{‚àí1}ùëà^T\). The \(Œ£^{‚àí1}\) is easy to compute if we just take the reciprocal of the diagnals.</li>
</ul>

<p><strong>When $X$ is not invertible, the pseudoinverse form would still work</strong> (in a pseudo way), which is better than the closed form. In this case, the diagnol of $Œ£$ would have zero values. so \(X^+ = ùëâŒ£^{+}ùëà^T\), where the \(Œ£^{+}\) would be taking the non-zero values‚Äô reciprocal and keep the zero values zero.</p>

<p><strong>The closed form would have $O(n^3)$ complexity while the SVD method would have $O(n^2)$ computational complexity.</strong></p>

<p><strong>3.GD, SGD, Mini-Batch GD</strong></p>

<p>For regression problems, the loss function is convex. This means it will always have a global minimum.</p>

<p>When we are using GD, we need to scale our data first, or the converging process would be longer. (if one feature is smaller, it would take a larger step to achieve similar decress in loss function as other features.)</p>

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/mlnew1-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/mlnew1-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/mlnew1-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/mlnew1.png">

  </picture>

  

</figure>

<p>We will calculate the gradient for each of the parameters, and take a step down.</p>

<ul>
  <li>Note: the gradient is pointing uphill.</li>
</ul>

\[\theta^{new}=\theta - r * \triangledown MSE(\theta)\]

<p>The SGD would take longer to reach global minimum, and bounce around it as new data point being added. But it allows the algorithm to jump out of local minimum and reach the global minimum.</p>

<blockquote>

**learning schedule**: If the learning rate is reduced too quickly, you may get stuck in a local minimum, or even end up frozen halfway to the minimum. If the learning rate is reduced too slowly, you may jump around the minimum for a long time and end up with a suboptimal solution if you halt training too early.

</blockquote>

<p>The training instances of SGD must be independent and identically distributed (IID)</p>

<blockquote>

If you do not do this, for example if the instances are sorted by label, then SGD will start by **optimizing for one label, then the next, and so on, and it will not settle close to the global minimum.**

</blockquote>

<p><strong>4. Extension: Polynomial Regression</strong></p>

<ul>
  <li>Overfitting and underfitting</li>
  <li>Trade off between variance and bias</li>
</ul>

<h3 id="regularized-linear-model">Regularized Linear Model</h3>

<p><strong>1. Ridge Regression</strong></p>

<ul>
  <li>regularization term: \(\alpha\sum^n\theta^2\), added to cost function. (It should not be added into evaluation function, as by objective, we aimed at decreasing the loss.)</li>
  <li>The bias term \(Œ∏_0\) is not regularized.</li>
  <li>The input data need to be regularized.</li>
  <li>
\[\hat Œ∏=(X^TX+\alpha I)^{‚àí1} X^T y\]
  </li>
</ul>

<p><strong>2. Lasso Regression</strong></p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>regularization term: $$\alpha\sum^n</td>
          <td>\theta</td>
          <td>$$.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Tends to completely elimi‚Äê nate the weights of the least important features, and return sparse outcome.</li>
  <li>\(\theta^T\theta = Constraint = C\), when \(\alpha\) goes up, \(C\) goes down.</li>
</ul>

<blockquote>

For univariate linear regression or linear regression with uncorrelated features + lasso regularization, there is a closed-form solution. For more generalized forms of regression such as linear regression with correlated features or logistic regression, there is **no closed-form solution** of the lasso-regularized version. [Source](https://www.quora.com/Is-there-a-closed-form-solution-to-LASSO-regression)

</blockquote>

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/mlnew2-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/mlnew2-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/mlnew2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/mlnew2.png">

  </picture>

  

</figure>

<p><strong>3. Elastic Net</strong></p>

<p>Combination of Ridge Regression and Lasso Regression.</p>

<blockquote>

Ridge is a good default, but if you suspect that only a few features are actually useful, you should **prefer Lasso or Elastic Net over ridge** since they tend to reduce the useless features‚Äô weights down to zero as we have discussed. In general, **Elastic Net is preferred over Lasso** since Lasso may behave **erratically** when the number of features is greater than the number of training instances or when several features are **strongly correlated**.

</blockquote>

<p><strong>4. Early Stopping</strong></p>

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/mlnew3-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/mlnew3-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/mlnew3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/mlnew3.png">

  </picture>

  

</figure>

<p>Using GD, and stop when validation error goes up. For SGD and Mini Batch, the curve would not be that smooth. So we can stop when validation error goes up for a while.</p>

<h3 id="logistic-regression">Logistic Regression</h3>

<ul>
  <li>
    <p>Form: \(\hat p = \sigma(X^T\theta)\), where \(\sigma(t)=\frac{1}{1+exp^{-t}}\).</p>
  </li>
  <li>
    <p>t is called logit, \(logit(p)=log(\frac{p}{1-p})\),is the inverse of logistic function. \(t = logit(\hat p)\).</p>
  </li>
  <li>
    <p>Logit is also called log-odds, as it is the log of the ratio between the estimated probability for the positive class and the estimated probability for the negative class.</p>
  </li>
  <li>
    <p>MLE: \(likelihood = \hat y * y + (1 ‚Äì \hat y) * (1 ‚Äì y)\),</p>

    <ul>
      <li>This function will always return a large probability when the model is close to the matching class value, and a small value when it is far away, for both y=0 and y=1 cases. <a href="https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/" target="_blank" rel="noopener noreferrer">Source</a>
</li>
    </ul>
  </li>
  <li>
    <p>log-likelihood = \(likelihood = log(\hat y) * y + log(1 ‚Äì \hat y) * (1 ‚Äì y)\)</p>

    <ul>
      <li>equivalent to cross-entropy</li>
      <li>Note: Cross entropy = \(-(log(q_{class0}) * p_{class0} + log(q_{class1}) * p_{class1})\)</li>
    </ul>
  </li>
  <li>
    <p>Cost function: \(log loss=-\frac{1}{m}\sum^m_{i=1}(y^ilog(\hat p^i)+(1-y^i)log(\hat (1-p^i)))\)</p>

    <ul>
      <li>No closed form.</li>
    </ul>
  </li>
  <li>
    <p>Linear decision boundary</p>
    <ul>
      <li>It is the the set of points x such that \(\theta_0 + \theta_1x_1 + \theta_2x_2 = 0\), which defines a straight line.</li>
    </ul>
  </li>
</ul>

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/mlnew4-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/mlnew4-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/mlnew4-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/mlnew4.png">

  </picture>

  

</figure>

<h3 id="softmax-regression">Softmax Regression</h3>

<p>Softmax function: it computes the exponential of every score and normalizes them by dividing the sum of all the exponentials.</p>

\[\sigma(p_i) = \frac{e^{p_i}}{\sum^k_{i=1} e^{p_i}}\]

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    ¬© Copyright 2022 Manqiu  L..
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
