<!DOCTYPE html>
<html lang="en">

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Manqiu  L.


  | Dimensionality Reduction, Kernel PCA and LDA

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒŒ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://manqiul.github.io/blog/2022/pca/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://manqiul.github.io/">
       <span class="font-weight-bold">Manqiu</span>   L.
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/courses/">
                Courses
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/experiences/">
                Experiences
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item dropdown ">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Interests
                
              </a>
              <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
              
              
                <a class="dropdown-item" href="/photography/">Photography</a>
              
              
              
                <div class="dropdown-divider"></div>
              
              
              
                <a class="dropdown-item" href="/reading/">Reading</a>
              
              
              </div>
          </li>
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      





<div class="post">

  <header class="post-header">
    <h2 class="post-title">Dimensionality Reduction, Kernel PCA and LDA</h2>
    <p class="post-meta">January 11, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
      
      Â  Â· Â 
        
        <a href="/blog/tag/machinelearning">
          <i class="fas fa-hashtag fa-sm"></i> machinelearning</a> Â 
          
      

      
      Â  Â· Â 
        
        <a href="/blog/category/notes">
          <i class="fas fa-tag fa-sm"></i> notes</a> Â 
          
      

    </p>
  </header>

  <article class="post-content">
    <h2 id="kernel-pca">Kernel PCA</h2>

<p>PCA is a linear method. That is it can only be applied to datasets which are linearly separable.Kernel PCA uses a kernel function to project dataset into a higher dimensional feature space, where it is linearly separable <a href="https://www.geeksforgeeks.org/ml-introduction-to-kernel-pca/" target="_blank" rel="noopener noreferrer">source</a></p>

<p><code class="language-plaintext highlighter-rouge">Why High Dimension?</code>: VC (Vapnik-Chervonenkis) theory tells us
that often mappings which take us into a
higher dimensional space than the
dimension of the input space provide us
with greater classification power <a href="http://www.cs.haifa.ac.il/~rita/uml_course/lectures/KPCA.pdf" target="_blank" rel="noopener noreferrer">source</a></p>

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/lda5-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/lda5-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/lda5-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/lda5.png">

  </picture>

  

</figure>

<p>But if we simply map the data into higher dimensions, the computation cost would be also high. Why?:</p>

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/lda6-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/lda6-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/lda6-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/lda6.png">

  </picture>

  

</figure>

<p>Here comes the <code class="language-plaintext highlighter-rouge">Kernel Trick</code>: Get the benefit of high-dimension, but avoid the computation burden brought by high dimensions.</p>

<p>Given any algorithm that can be expressed solely <strong>in terms of dot products</strong>, this trick allows us to construct different nonlinear versions of it. And it will save compuational cost. Why?</p>

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/lda7-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/lda7-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/lda7-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/lda7.png">

  </picture>

  

</figure>

<p>Popular Kernel funnctions:</p>

<p>Polynomial kernel \(k(x,y)=(x^Ty+1)^d\).</p>

<p>It looks not only at the given features of input samples to determine their similarity, but also combinations of these. With \(n\) original features and \(d\) degrees of polynomial, the polynomial kernel yields \(n^d\) expanded features.</p>

<table>
  <tbody>
    <tr>
      <td>RBF kernel, also known as Gaussian kernel $$k(x,y)=e^{-\gamma</td>
      <td>Â </td>
      <td>x-y</td>
      <td>Â </td>
      <td>^2}$$.</td>
    </tr>
  </tbody>
</table>

<p>There is an infinite number of dimensions in the feature space because it can be expanded by the Taylor Series. The \(\gamma\) parameter defines how much influence a single training example has. The larger it is, the closer other examples must be to be affected</p>

<p><code class="language-plaintext highlighter-rouge">Math</code>: <a href="http://www.cs.haifa.ac.il/~rita/uml_course/lectures/KPCA.pdf" target="_blank" rel="noopener noreferrer">source</a></p>

<p><code class="language-plaintext highlighter-rouge">Limitation</code>: Overfitting when we mapping features into higher dimensions.</p>

<h2 id="linear-discriminant-analysis">Linear Discriminant Analysis</h2>

<p><code class="language-plaintext highlighter-rouge">Main Idea</code>: With tagged data, we can perform supervised dimensionality reduction. The LDA is to project all the data points on to a lower dimension, which seperates the classes as far as possible. <a href="https://medium.com/analytics-vidhya/linear-discriminant-analysis-explained-in-under-4-minutes-e558e962c877" target="_blank" rel="noopener noreferrer">source1</a>, <a href="https://medium.com/@viveksalunkhe80/linear-discriminant-analysis-2b7bfc409f9b" target="_blank" rel="noopener noreferrer">source2</a></p>

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/lda1-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/lda1-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/lda1-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/lda1.jpeg">

  </picture>

  

</figure>

<p>We achieve this by maximizing <strong>between-class variance</strong> and minimizing <strong>within-class variance</strong>.</p>

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/lda2-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/lda2-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/lda2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/lda2.png">

  </picture>

  

</figure>

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/lda3-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/lda3-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/lda3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/lda3.png">

  </picture>

  

</figure>

<p>Fisher defined the separation between these two distributions to be the ratio of the variance between the classes to the variance within the classes.</p>

<p><code class="language-plaintext highlighter-rouge">What we can get from LDA?</code>:</p>

<p>Low-dimensional representations of data points, where each class could be largely seperated.</p>

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/lda3-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/lda3-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/lda3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/lda3.png">

  </picture>

  

</figure>

<p><code class="language-plaintext highlighter-rouge">Use Case</code>: Reduce the number of features to a more manageable number before classification</p>

<p><code class="language-plaintext highlighter-rouge">Limitation</code> LDA will fail if discriminatory information is not in the mean but in the variance of the data (That is, the classes share mean value.). We can use <strong>Non-linear Discriminant Analysis</strong>:<a href="https://medium.com/@viveksalunkhe80/linear-discriminant-analysis-2b7bfc409f9b" target="_blank" rel="noopener noreferrer">source</a></p>

<ol>
  <li>Quadratic Discriminant Analysis (QDA): Each class uses its own estimate of variance (or covariance when there are multiple input variables).</li>
  <li>Flexible Discriminant Analysis (FDA): Where non-linear combinations of inputs is used such as splines.</li>
  <li>Regularized Discriminant Analysis (RDA): Introduces regularization into the estimate of the variance (actually covariance), moderating the influence of different variables on LDA.</li>
</ol>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    Â© Copyright 2022 Manqiu  L..
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
